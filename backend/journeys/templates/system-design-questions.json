{
  "journey": {
    "id": "system-design-practice",
    "title": "System Design Interview Practice",
    "description": "Endless system design practice with 25+ real interview questions. Answer questions, reveal guided answers (like flashcards), and keep practicing until you've seen all questions.",
    "type": "flashcard",
    "is_configurable": false,
    "question_category": "system-design",
    "created_by": "system"
  },
  "questions": [
    {
      "id": "q-001",
      "question_key": "design-url-shortener",
      "title": "Design a URL Shortening Service (like bit.ly)",
      "prompt": "Design a URL shortening service that allows users to create short URLs that redirect to long URLs. Consider scalability, collision handling, and analytics.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Generate short, unique URLs from long URLs\n   - Redirect short URL to original long URL\n   - Custom short URL support (optional)\n   - Analytics (clicks, referrer, location)\n   - URL expiration\n\n2. Non-Functional Requirements:\n   - High availability\n   - Low latency (high redirect performance)\n   - Unique URLs\n   - Scalable\n\n3. High-Level Design:\n   - API Gateway\n   - Encoding Service (convert long URL to short code)\n   - Database (mapping short to long URLs)\n   - Cache (Redis for hot URLs)\n   - Analytics Service\n   - Cleanup/Expiration Service\n\n4. Encoding Strategy:\n   - Base62 encoding with counter (deterministic)\n   - OR random string generation with collision detection\n   - URL length: 6-8 characters gives ~56 trillion combinations (base62)\n\n5. Database Design:\n   - Schema: ShortURL(id, short_code, long_url, user_id, created_at, expiration_at, click_count)\n   - Primary key on short_code for fast lookup\n   - Index on user_id for user-specific queries\n\n6. Scalability:\n   - Distributed cache (Redis) for hot URLs\n   - Database replication\n   - Consistent hashing for distributed encoding\n   - Partition by short_code hash\n\n7. Challenges:\n   - Collision handling (reserve codes, retry with different algorithm)\n   - Distributed ID generation (Zookeeper for counter)\n   - Analytics at scale (async logging, batch processing)\n   - Cache invalidation (TTL-based expiration)\n\n8. Estimation:\n   - Traffic: 100M URL creations/month, 1B redirects/month\n   - Storage: ~500GB for URL mappings\n   - QPS: Redirects ~385K/s, Creation ~39K/s",
      "category": "system-design",
      "difficulty": "medium"
    },
    {
      "id": "q-002",
      "question_key": "design-twitter",
      "title": "Design Twitter (or a Social Media Feed)",
      "prompt": "Design a social media platform like Twitter where users can post tweets, follow others, and see a personalized feed of tweets from people they follow.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Post tweets (text, media)\n   - Follow/unfollow users\n   - Home feed (tweets from followed users + own tweets)\n   - User profile/timeline\n   - Like/retweet tweets\n   - Search tweets\n   - Notifications\n\n2. Non-Functional Requirements:\n   - High availability\n   - Low latency for feed generation\n   - Consistency (eventual)\n   - Scalability (millions of users)\n\n3. High-Level Design:\n   - API Gateway\n   - User Service\n   - Tweet Service\n   - Feed Service (most complex)\n   - Notification Service\n   - Search Service (Elasticsearch)\n\n4. Data Model:\n   - User(id, username, name, bio, followers_count, following_count)\n   - Tweet(id, user_id, content, created_at, like_count, retweet_count)\n   - Follow(follower_id, following_id)\n   - Like(user_id, tweet_id)\n   - Retweet(user_id, tweet_id)\n\n5. Feed Generation (Critical):\n   Push Model (Fan-out):\n   - When user posts, push to all followers' feeds\n   - Pros: Fast feed read, minimal computation\n   - Cons: Expensive for users with millions of followers\n   \n   Pull Model:\n   - When user requests feed, pull tweets from all followed users\n   - Pros: Works well for accounts with many followers\n   - Cons: Slow feed generation\n   \n   Hybrid:\n   - Use push for normal users (<10K followers)\n   - Use pull for celebrities (millions of followers)\n\n6. Storage:\n   - SQL for users and relationships (strong consistency)\n   - NoSQL for tweets (denormalized, high write throughput)\n   - Cache (Redis) for feeds\n   - Graph DB option for relationships\n\n7. Scaling:\n   - Cache home feeds in Redis (TTL ~1 hour)\n   - Use message queues (Kafka) for async operations\n   - Partition by user_id\n   - Read replicas for user/tweet queries\n   - Denormalize tweet data in feed cache\n\n8. Search:\n   - Elasticsearch for full-text tweet search\n   - Index: tweet_id, user_id, content, created_at, hashtags\n   - Inverted index on content\n\n9. Notification:\n   - Async event-driven\n   - Kafka topics for like/follow/retweet events\n   - Notification service consumes and stores in DB/cache\n\n10. Estimation:\n    - Users: 500M\n    - Daily active: 100M\n    - Tweets/day: 500M\n    - Average followers: 100\n    - Average feed read: 10x per day",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-003",
      "question_key": "design-youtube",
      "title": "Design YouTube",
      "prompt": "Design a video streaming platform like YouTube. Consider video upload, transcoding, streaming, recommendations, and search.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Upload videos\n   - Transcode videos (multiple quality/format)\n   - Stream videos (adaptive bitrate)\n   - Search videos\n   - Recommendations\n   - Like/comment/subscribe\n   - Playlist creation\n   - Watch history\n\n2. Non-Functional Requirements:\n   - High availability\n   - Low latency streaming\n   - Supports different devices (mobile, desktop, TV)\n   - Scalable storage (exabytes of video)\n\n3. High-Level Design:\n   - API Gateway\n   - Upload Service\n   - Transcoding Service\n   - Streaming Service (CDN)\n   - Search Service (Elasticsearch)\n   - Recommendation Engine\n   - Video DB + Metadata DB\n   - Message Queue (Kafka)\n\n4. Upload & Transcoding:\n   - Direct upload to object storage (S3/GCS)\n   - Post upload notification to message queue\n   - Transcoding workers pick up jobs\n   - Multi-format transcoding (MP4, WebM, etc.)\n   - Multiple bitrates (240p-4K)\n   - Store transcoded videos in object storage\n   - Update DB with transcoding status\n\n5. Streaming:\n   - Use CDN for global distribution\n   - Adaptive bitrate streaming (HLS/DASH)\n   - Client adjusts quality based on bandwidth\n   - Partial content requests (HTTP 206 Range)\n   - Cache popular videos at edge nodes\n\n6. Storage:\n   - Object storage (S3) for video files\n   - SQL DB for metadata (title, description, etc.)\n   - NoSQL for denormalized user data (views, comments)\n   - Cache (Redis) for hot data (trending, recommendations)\n   - Logs in HDFS/Data Lake for analytics\n\n7. Search:\n   - Elasticsearch for full-text search\n   - Index: video_id, title, description, tags, channel\n   - Autocomplete using trie/prefix tree\n\n8. Recommendations:\n   - Collaborative filtering\n   - Content-based filtering\n   - Hybrid approach\n   - ML model trained on:\n     * Watch history\n     * Watch duration\n     * Likes/dislikes\n     * Search history\n\n9. Database Sharding:\n   - Shard by user_id for user data\n   - Shard by video_id for video metadata\n   - Separate DB for each shard\n\n10. Estimation:\n    - Videos uploaded/day: 500 hours\n    - Video storage: 1 PB (with transcoding)\n    - Streaming QPS: 1M+ concurrent viewers\n    - Average video size: 1GB (1hr, 1080p)",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-004",
      "question_key": "design-uber",
      "title": "Design Uber (Ride-Sharing System)",
      "prompt": "Design an Uber-like ride-sharing system. Consider matching drivers and riders, real-time location tracking, ETA calculation, and payment.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Request ride\n   - Match with nearby driver\n   - Track driver location in real-time\n   - Calculate ETA\n   - Ride history\n   - Payment\n   - Ratings/reviews\n\n2. Non-Functional Requirements:\n   - High availability\n   - Low latency matching\n   - Real-time updates\n   - Geospatial queries\n   - Scalability (millions of users)\n\n3. High-Level Design:\n   - API Gateway\n   - Ride Request Service\n   - Matching Service (critical)\n   - Location Service (track driver location)\n   - ETA Service\n   - Payment Service\n   - Notification Service\n   - Map Service (Google Maps API)\n\n4. Matching Algorithm:\n   - Find nearby drivers (within X miles)\n   - Consider driver rating/acceptance rate\n   - Consider ride duration/destination\n   - Use geospatial index (Quadtree, KD-tree, or geohashing)\n   - Consider surge pricing\n   - Assign to highest-ranked driver\n\n5. Location Tracking:\n   - Drivers send location update every 3-5 seconds\n   - Store in location service (Redis with geospatial index)\n   - Publish to event stream (Kafka) for real-time updates\n   - Push to rider's app via WebSocket\n\n6. ETA Calculation:\n   - Use Google Maps API for routing\n   - Consider traffic patterns (ML model)\n   - Cache common routes\n   - Update as conditions change\n\n7. Database Design:\n   - User(id, name, phone, email)\n   - Driver(id, license, vehicle, rating, status)\n   - Ride(id, rider_id, driver_id, pickup, dropoff, status, created_at)\n   - Location(driver_id, lat, long, updated_at) - in Redis/cache\n\n8. Storage:\n   - SQL DB for user/driver/ride data\n   - Redis/Memcached for location (hot data)\n   - Message queue (Kafka) for events\n   - Time-series DB for analytics\n\n9. Challenges:\n   - Concurrency: multiple riders requesting simultaneously\n   - Consistency: driver accepted ride but then went offline\n   - Geospatial queries at scale\n   - Handling surge pricing\n   - Driver availability updates\n\n10. Estimation:\n    - 1M rides/day\n    - 500K drivers\n    - 5M users\n    - Location updates: 500K drivers * 1 update/3s = 166K events/s",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-005",
      "question_key": "design-cache",
      "title": "Design a Distributed Cache System",
      "prompt": "Design a distributed cache system similar to Redis or Memcached that can handle millions of concurrent requests with high availability.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Get/Set/Delete operations\n   - Expiration/TTL\n   - Support multiple data types (string, list, set, hash, zset)\n   - Persistence (optional)\n   - Replication\n   - Sharding\n\n2. Non-Functional Requirements:\n   - High throughput\n   - Low latency\n   - High availability\n   - Consistency\n   - Scalability\n\n3. Architecture:\n   - Single-threaded event loop (Redis approach)\n   - In-memory data structure\n   - Persistence layer (RDB snapshots, AOF logs)\n   - Replication (master-slave)\n   - Cluster mode (horizontal scaling)\n\n4. In-Memory Data Structures:\n   - Hash table (primary index)\n   - Skip list (for sorted sets)\n   - Doubly-linked list (for eviction)\n   - Radix tree (for commands with pattern matching)\n\n5. Eviction Policies:\n   - LRU (Least Recently Used) - tracks access time\n   - LFU (Least Frequently Used) - tracks frequency\n   - TTL-based\n   - Random\n   - FIFO\n\n6. Persistence:\n   RDB Snapshots:\n   - Periodic snapshots of entire dataset\n   - Fast recovery\n   - Cons: Data loss between snapshots\n   \n   AOF (Append-Only File):\n   - Log every write operation\n   - More durable\n   - Slower writes\n   - Can replay to recover\n\n7. Replication:\n   - Master accepts writes\n   - Slaves replicate data from master\n   - Asynchronous replication\n   - Handle slave lag (eventually consistent)\n\n8. Clustering:\n   - Hash ring (consistent hashing)\n   - Partition data across nodes\n   - Handle node failures\n   - Redistribution on node addition/removal\n\n9. Network Protocol:\n   - Binary protocol (Redis RESP)\n   - Stateful TCP connections\n   - Pipelining support\n   - Pub/Sub support\n\n10. Challenges:\n    - Memory management/eviction\n    - Network latency\n    - Split-brain in cluster\n    - Handling hot keys\n    - Memory overflow\n\n11. Optimization:\n    - Connection pooling\n    - Batch operations\n    - Bloom filters for non-existent keys\n    - Compression",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-006",
      "question_key": "design-database",
      "title": "Design a Relational Database",
      "prompt": "Design a relational database system that supports ACID properties, indexing, query optimization, and replication at scale.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - SQL queries (SELECT, INSERT, UPDATE, DELETE)\n   - ACID transactions\n   - Indexing (B-tree, hash indexes)\n   - Joins\n   - Aggregations\n   - Replication\n   - Backup/recovery\n\n2. Architecture Layers:\n   - Storage Engine\n   - Query Processor\n   - Query Optimizer\n   - Transaction Manager\n   - Buffer Manager\n   - Replication Manager\n\n3. Storage Engine:\n   - B-tree index for range queries\n   - Hash index for point lookups\n   - Write-ahead logging (WAL) for durability\n   - Page-based storage (4KB pages)\n   - Buffer pool/cache\n\n4. Transaction Management:\n   - ACID properties:\n     * Atomicity: All-or-nothing (WAL + commit protocol)\n     * Consistency: Schema constraints, triggers\n     * Isolation: Isolation levels (serializable, repeatable-read, read-committed)\n     * Durability: Write to disk before commit\n   \n   - Locking:\n     * Read locks (shared)\n     * Write locks (exclusive)\n     * Lock escalation\n     * Deadlock detection\n   \n   - Optimistic Concurrency:\n     * MVCC (Multi-Version Concurrency Control)\n     * Each transaction sees snapshot\n     * No locks needed\n\n5. Query Execution:\n   - Parser: SQL → AST\n   - Validator: Check schema\n   - Planner: Generate execution plans\n   - Optimizer: Choose best plan\n     * Cost estimation\n     * Statistics on data distribution\n     * Join order optimization\n   - Executor: Execute plan\n\n6. Indexing:\n   - B-tree: Range queries, sorting\n   - Hash: Equality queries\n   - Bitmap: Low cardinality columns\n   - Full-text: Text search\n   - Covering indexes: All columns in index\n\n7. Replication:\n   - Master-slave (read replicas)\n   - Binary log shipping\n   - Statement-based vs row-based\n   - Semi-synchronous replication\n\n8. Sharding:\n   - Range-based sharding\n   - Hash-based sharding\n   - Directory-based sharding\n   - Handle resharding\n\n9. Challenges:\n   - Query optimization complexity\n   - Join performance across shards\n   - Distributed transactions\n   - Hot spots\n   - Schema changes at scale",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-007",
      "question_key": "design-search-engine",
      "title": "Design a Search Engine",
      "prompt": "Design a search engine like Google that can crawl the web, index pages, and return relevant results for user queries.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Web crawling\n   - Indexing pages\n   - Full-text search\n   - Ranking/relevance\n   - Suggest corrections (spell check)\n   - Search analytics\n\n2. Non-Functional Requirements:\n   - Handle billions of pages\n   - Fast search results (<100ms)\n   - Freshness (crawl frequently)\n   - Scalability\n\n3. High-Level Components:\n   - Web Crawler\n   - Indexer\n   - Storage (distributed filesystem)\n   - Query Processor\n   - Ranking Engine\n   - Search Results Service\n\n4. Web Crawling:\n   - Start with seed URLs\n   - BFS/DFS to traverse pages\n   - Respect robots.txt\n   - Rate limiting\n   - Distributed crawling (many crawler nodes)\n   - URL frontier (queue of URLs to crawl)\n   - Handle redirects, duplicates\n\n5. Indexing:\n   - Extract text from HTML\n   - Tokenization\n   - Remove stop words\n   - Stemming/lemmatization\n   - Build inverted index\n   - Store posting lists (list of documents containing word)\n\n6. Inverted Index:\n   - Map: word → [document_id, position, frequency, ...]\n   - Supports full-text search\n   - Store on disk (too large for memory)\n   - Use compression techniques\n   - Support boolean queries (AND, OR, NOT)\n\n7. Ranking Algorithm:\n   - PageRank: Link structure importance\n   - TF-IDF: Term frequency-inverse document frequency\n   - BM25: Probabilistic ranking\n   - Consider:\n     * Query term frequency\n     * Document length normalization\n     * Link quality\n     * Freshness\n     * User signals (CTR)\n\n8. Storage:\n   - Distributed filesystem (HDFS)\n   - Store inverted index\n   - Store page metadata (URL, title, snippet, last crawled)\n   - Bit vectors for fast filtering\n\n9. Query Processing:\n   - Parse query\n   - Spell correction\n   - Query expansion (synonyms)\n   - Find matching documents using inverted index\n   - Rank documents\n   - Return top K results\n\n10. Challenges:\n    - Duplicate detection (fingerprinting)\n    - Spam detection\n    - Keeping index fresh\n    - Handling query ambiguity\n    - Personalization\n\n11. Scale Estimation:\n    - Index size: 200 billion pages × 10KB avg = 2 exabytes\n    - QPS: billions of queries per day",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-008",
      "question_key": "design-key-value-store",
      "title": "Design a Key-Value Store like DynamoDB",
      "prompt": "Design a distributed key-value store that guarantees consistency, availability, and partition tolerance while scaling to handle massive data.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Get/Put/Delete operations\n   - Strong consistency OR eventual consistency option\n   - High availability (multiple replicas)\n   - Durability\n   - Atomic operations\n\n2. CAP Theorem Analysis:\n   - Consistency: All replicas return same data\n   - Availability: System remains operational despite failures\n   - Partition tolerance: System works despite network partitions\n   - Can only guarantee 2 of 3\n   - DynamoDB chooses: Availability + Partition Tolerance\n\n3. Architecture:\n   - Distributed hash table\n   - Consistent hashing for node mapping\n   - Replication (typically 3 copies)\n   - Versioning (vector clocks)\n   - Anti-entropy (background sync)\n\n4. Partitioning:\n   - Consistent hashing with virtual nodes\n   - Partition key → node mapping\n   - Handle node joins/leaves without reshuffling all data\n   - Add virtual nodes for load balancing\n\n5. Replication:\n   - Primary node + 2 replica nodes (or N-1 replicas for N nodes)\n   - Read from any replica (faster reads)\n   - Write to all replicas (or quorum)\n   - Handle replica failures\n   - Gossip protocol for failure detection\n\n6. Consistency Models:\n   - Strong Consistency: Latest write visible immediately\n   - Eventual Consistency: Updates propagate slowly\n   - Quorum-based:\n     * Read quorum + Write quorum\n     * If R + W > N, strong consistency\n     * R=1, W=N: Fast reads, slow writes\n     * R=N, W=1: Slow reads, fast writes\n\n7. Conflict Resolution:\n   - Last-write-wins: Use timestamp (clock skew issues)\n   - Vector clocks: Track causality\n   - Application-level conflict resolution\n   - Merge function (like in Git)\n\n8. Persistence:\n   - Write-ahead log (WAL)\n   - Periodic snapshots\n   - SSD/HDD storage\n   - Compression\n   - Bloom filters for non-existent keys\n\n9. Data Structure:\n   - Skip list or B-tree for range queries\n   - Hash table for fast lookup\n   - Bloom filter for negative queries\n   - LSM tree (Log-Structured Merge tree) for write optimization\n\n10. Failure Handling:\n    - Gossip protocol for node discovery\n    - Merkle trees for efficient reconciliation\n    - Hinted handoff for temporary failures\n    - Read repair for data inconsistency\n\n11. Challenges:\n    - Network partitions\n    - Handling clock skew\n    - Balancing load across partitions\n    - Hot spots\n    - Distributed garbage collection",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-009",
      "question_key": "design-message-queue",
      "title": "Design a Distributed Message Queue",
      "prompt": "Design a message queue system like Kafka or RabbitMQ that can handle high throughput, ensure message delivery, and support multiple consumers.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Producers send messages\n   - Brokers store messages\n   - Consumers receive messages\n   - Topics/Queues for message categorization\n   - Subscriptions (multiple consumers per topic)\n   - Message ordering (per partition)\n   - Acknowledgment/Delivery guarantees\n\n2. Message Delivery Guarantees:\n   - At-most-once: Message sent once, no retries (may lose)\n   - At-least-once: Message retried until ack (may duplicate)\n   - Exactly-once: Exactly one delivery (hard to achieve)\n\n3. Architecture:\n   - Producers: Send messages\n   - Brokers: Cluster of message queue nodes\n   - Consumers: Process messages\n   - Zookeeper/Etcd: Cluster coordination\n\n4. Topic & Partitioning:\n   - Topics logically group messages\n   - Partitions for parallelism\n   - Messages in partition are ordered\n   - Each partition has leader + replicas\n   - Partition key determines which partition\n\n5. Storage:\n   - Append-only log per partition\n   - Write to disk sequentially (fast)\n   - Keep in memory cache\n   - Retention policy (time-based or size-based)\n   - Compression (snappy, gzip)\n\n6. Consumer Groups:\n   - Multiple consumers process same topic\n   - Each partition assigned to one consumer (avoid duplicate processing)\n   - Rebalancing when consumer joins/leaves\n   - Offset management (track position in log)\n\n7. Replication:\n   - In-Sync Replicas (ISR): Replicas caught up with leader\n   - Leader: Handles all reads/writes\n   - Followers: Replicate from leader\n   - Min ISR for acknowledgment\n\n8. Failure Handling:\n   - Leader election (pick from ISR)\n   - Handle broker failure\n   - Handle consumer failure (rebalancing)\n   - Data durability (min replication factor)\n\n9. Performance Optimization:\n   - Batching: Batch multiple messages in single request\n   - Compression: Reduce network bandwidth\n   - Async sends: Don't wait for ack\n   - Zero-copy: Direct memory transfer\n   - Sequential writes to disk\n\n10. Ordering:\n    - Messages in partition are ordered\n    - Consumers process in order (single consumer per partition)\n    - Cross-partition ordering: Not guaranteed\n\n11. Challenges:\n    - Exactly-once processing (requires idempotency + deduplication)\n    - Hot partitions\n    - Consumer lag monitoring\n    - Schema evolution\n    - Back-pressure handling",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-010",
      "question_key": "design-rate-limiter",
      "title": "Design a Rate Limiter",
      "prompt": "Design a rate limiting system that prevents abuse and ensures fair usage of an API. Support different rate limiting strategies.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Limit requests per time window\n   - Support different strategies\n   - Return rate limit info in headers\n   - Block or queue excess requests\n   - Per user/IP/API key limits\n   - Distributed (across multiple servers)\n\n2. Rate Limiting Strategies:\n\n   Token Bucket:\n   - Bucket has capacity N tokens\n   - Refill R tokens per second\n   - Each request uses 1 token\n   - Reject if no tokens\n   - Allows burst (up to bucket capacity)\n   \n   Leaky Bucket:\n   - Fixed rate of requests (smooth)\n   - Queue excess requests\n   - Discard if queue full\n   - Output rate constant\n   \n   Sliding Window Log:\n   - Keep log of requests\n   - For new request, count requests in last time window\n   - Reject if count ≥ limit\n   - Cons: Uses lots of memory\n   \n   Sliding Window Counter:\n   - Similar to sliding window log\n   - But store counts, not individual requests\n   - More efficient\n   - Slightly inaccurate (edge cases)\n\n3. Single Machine Implementation:\n   - Token Bucket:\n     * last_refill_time: Last time tokens were added\n     * current_tokens: Current token count\n     * On request: Add (now - last_refill_time) * refill_rate tokens\n     * Use 1 token, respond success or reject\n   \n   - Use concurrent map/dictionary\n   - Clean up old entries\n   - Thread-safe operations\n\n4. Distributed Implementation:\n   - Redis:\n     * Store tokens in key-value store\n     * Increment/decrement atomically\n     * Use Lua scripts for atomic operations\n     * Set TTL for auto-cleanup\n   \n   - Algorithm:\n     * GET key → current_tokens\n     * If current_tokens < 1: REJECT\n     * DECREMENT key\n     * SET TTL\n     * ACCEPT\n\n5. Redis Lua Script Example:\n   ```\n   SCRIPT_CONTENT = \"\n     local key = KEYS[1]\n     local limit = ARGV[1]\n     local refill_rate = ARGV[2]\n     local now = tonumber(ARGV[3])\n     \n     local current = redis.call('GET', key)\n     if current == false then\n       redis.call('SET', key, limit)\n       return 1\n     end\n     \n     -- Calculate tokens\n     local last_refill = redis.call('GET', key .. ':refill')\n     local tokens = current + (now - last_refill) * refill_rate\n     tokens = math.min(tokens, limit)\n     \n     if tokens < 1 then\n       return 0\n     end\n     \n     redis.call('SET', key, tokens - 1)\n     redis.call('SET', key .. ':refill', now)\n     return 1\n   \"\n   ```\n\n6. Multiple Rate Limits:\n   - Per IP, per user, per API endpoint\n   - Hierarchical limits\n   - Combine: Min(IP limit, user limit, global limit)\n\n7. Response Headers:\n   - X-RateLimit-Limit: Request limit\n   - X-RateLimit-Remaining: Requests remaining\n   - X-RateLimit-Reset: Time when limit resets\n   - Retry-After: When to retry (on 429)\n\n8. Challenges:\n   - Distributed clock skew\n   - Network latency to Redis\n   - Handling Redis failures\n   - Hot users (heavy traffic)\n   - Fairness across users\n\n9. Edge Cases:\n   - Clock goes backward\n   - Burst traffic\n   - Multiple requests in same millisecond\n   - Rate limit suddenly lowered",
      "category": "system-design",
      "difficulty": "medium"
    },
    {
      "id": "q-011",
      "question_key": "design-load-balancer",
      "title": "Design a Load Balancer",
      "prompt": "Design a load balancer that distributes incoming requests across multiple servers, handles failures, and supports different load balancing algorithms.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Distribute requests across servers\n   - Health checks\n   - Failover to healthy servers\n   - Support multiple algorithms\n   - Session persistence (sticky sessions)\n   - SSL/TLS termination\n\n2. Load Balancing Algorithms:\n\n   Round Robin:\n   - Distribute requests in circular order\n   - Pros: Simple, fair\n   - Cons: Doesn't account for server capacity\n   \n   Weighted Round Robin:\n   - Each server has weight\n   - Higher weight = more requests\n   - Pros: Account for capacity\n   \n   Least Connections:\n   - Send to server with fewest active connections\n   - Pros: Handles varying request durations\n   - Cons: Need to track connections\n   \n   IP Hash:\n   - Hash client IP → server\n   - Pros: Session persistence\n   - Cons: Uneven distribution, server removal breaks hash\n   \n   Consistent Hashing:\n   - Hash ring\n   - Add/remove servers with minimal disruption\n   - Pros: Better for distributed systems\n\n3. Architecture:\n   - Request arrives at LB\n   - LB selects server based on algorithm\n   - Forward request to server\n   - Forward response to client\n   - Track server health\n\n4. Health Checks:\n   - Periodic health checks (TCP, HTTP, custom)\n   - Mark server as healthy/unhealthy\n   - Remove unhealthy from rotation\n   - Probe interval: 5-10 seconds\n   - Mark unhealthy after N failed probes\n\n5. Session Persistence (Sticky Sessions):\n   - Option 1: IP hashing (can break if client IP changes)\n   - Option 2: Cookie-based (set cookie with server ID)\n   - Option 3: Store session data centrally (Redis)\n   - Pros: Client sees consistent state\n   - Cons: Reduces flexibility\n\n6. Layer 4 vs Layer 7 Load Balancing:\n   - Layer 4 (TCP/UDP): Fast, doesn't understand application\n   - Layer 7 (HTTP): Understand requests, enable advanced routing\n   - Layer 7 can route by URL path, hostname, headers\n\n7. Implementation:\n   - Single machine: In-memory data structures\n   - Distributed: Multiple LBs with coordination\n   - High availability:\n     * Multiple LBs (active-active or active-passive)\n     * Heartbeat between LBs\n     * Virtual IP (VIP) that follows active LB\n     * Floating IP for automatic failover\n\n8. Handling Connections:\n   - TCP connection terminates at LB\n   - LB opens new connection to backend server\n   - LB forwards data between connections\n   - Use non-blocking I/O for scalability\n\n9. SSL/TLS Termination:\n   - Decrypt at LB\n   - Forward unencrypted to backend (on internal network)\n   - Pros: Backend doesn't handle crypto\n   - Cons: Unencrypted internal traffic\n   - Alternative: Pass-through (LB doesn't decrypt)\n\n10. Connection Timeouts:\n    - Idle timeout (close if no data)\n    - Connection timeout (fail if can't connect)\n    - Request timeout\n\n11. Challenges:\n    - Handling millions of concurrent connections\n    - Connection state tracking\n    - Cross-zone latency\n    - Asymmetric routing\n    - DDoS mitigation\n\n12. Software Examples:\n    - HAProxy\n    - Nginx\n    - AWS ELB/ALB\n    - Google Cloud LB",
      "category": "system-design",
      "difficulty": "medium"
    },
    {
      "id": "q-012",
      "question_key": "design-notification-system",
      "title": "Design a Notification System",
      "prompt": "Design a system that sends notifications to users (email, SMS, push notifications) at scale with reliable delivery.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Send notifications via email, SMS, push\n   - Schedule notifications\n   - Retry on failure\n   - Track delivery status\n   - User preferences (opt-in/out)\n   - Notification templates\n\n2. Non-Functional Requirements:\n   - Reliability (ensure delivery)\n   - Scalability (millions of notifications)\n   - Low latency (deliver quickly)\n   - High availability\n\n3. High-Level Architecture:\n   - Notification Service (API)\n   - Message Queue (async processing)\n   - Workers (send notifications)\n   - External providers (SendGrid, Twilio, FCM)\n   - Database (track delivery)\n   - Retry mechanism\n\n4. Flow:\n   1. Application calls Notification Service\n   2. Create notification record in DB\n   3. Publish to message queue\n   4. Workers pick up from queue\n   5. Call external provider\n   6. Update delivery status\n   7. Retry on failure with exponential backoff\n\n5. Message Queue Design:\n   - Separate queues for email, SMS, push\n   - Priority queue (high priority notifications first)\n   - FIFO for ordered delivery (if needed)\n   - Dead letter queue for failed messages\n   - Use Kafka or RabbitMQ\n\n6. Worker Design:\n   - Multiple workers per queue\n   - Pick batch of messages (for efficiency)\n   - Send to external provider\n   - Handle timeouts/retries\n   - Update database with status\n   - Error handling and monitoring\n\n7. Retry Strategy:\n   - Exponential backoff: 1s, 2s, 4s, 8s...\n   - Max retry attempts: 3-5\n   - Max retry duration: 24 hours\n   - Different strategies per channel:\n     * Email: More retries (less time-sensitive)\n     * SMS: Fewer retries (time-sensitive)\n     * Push: Fewer retries (dropped is acceptable)\n\n8. Database Design:\n   - Notification(id, user_id, template_id, content, channel, status, created_at)\n   - NotificationStatus: pending, sent, failed, bounced\n   - Index on (user_id, created_at) for history\n   - Index on (status, created_at) for retry query\n   - Partition by date for old data cleanup\n\n9. Rate Limiting:\n   - Per user rate limit (don't spam)\n   - Per provider rate limit (API limits)\n   - Per domain rate limit (email)\n   - Implement token bucket algorithm\n\n10. Templates:\n    - Store reusable notification templates\n    - Support variables: {user_name}, {order_id}, etc.\n    - Validate placeholders\n    - Version templates\n\n11. User Preferences:\n    - Store opt-in/out per channel\n    - Store frequency preferences\n    - Respect do-not-disturb times\n    - Unsubscribe links\n\n12. Monitoring & Analytics:\n    - Track delivery rates per channel\n    - Track latency\n    - Alert on failures\n    - User engagement metrics (open rate for email)\n\n13. Challenges:\n    - Provider downtime (fallback providers)\n    - Deduplication (same notification sent twice)\n    - Ordering (notifications arrive in order)\n    - Exactly-once delivery (idempotency keys)\n    - Cost optimization (batch sends)",
      "category": "system-design",
      "difficulty": "medium"
    },
    {
      "id": "q-013",
      "question_key": "design-web-crawler",
      "title": "Design a Web Crawler",
      "prompt": "Design a distributed web crawler that can efficiently crawl the web at scale while respecting robots.txt and avoiding duplicates.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Crawl websites\n   - Extract links from pages\n   - Follow robots.txt\n   - Avoid duplicates\n   - Handle redirects\n   - Extract page content\n   - Update indices\n\n2. Non-Functional Requirements:\n   - Scalability (crawl billions of pages)\n   - Freshness (update frequently)\n   - Respect server load\n   - Fault tolerance\n\n3. Architecture:\n   - URL frontier (queue of URLs to crawl)\n   - DNS resolver cache\n   - Fetcher (download pages)\n   - Parser (extract links/content)\n   - Duplicate detector\n   - Robots.txt cache\n   - Storage\n\n4. URL Frontier (Most Critical):\n   - Priority queue (prioritize important URLs)\n   - Politeness: Don't overload single domain\n   - Freshness: Crawl frequently-updated sites more\n   - Capacity: Can grow very large\n   \n   Design:\n   - Back queue (hold URLs for many domains)\n   - Front queue (hold URLs for current domain being crawled)\n   - Multiple back queues (prevent starvation)\n   - Distribute domains across crawlers\n\n5. Duplicate Detection:\n   - Many pages are duplicates (different URLs, same content)\n   - Fingerprinting: Hash content to detect duplicates\n   - Bloom filter: Quick check for seen fingerprints\n   - Store fingerprints in distributed DB/cache\n\n6. Distributed Crawling:\n   - Multiple crawler nodes\n   - Partition domain space: crawler 1 gets A-D, crawler 2 gets E-H, etc.\n   - OR: Hash domain → crawler\n   - Single crawler per domain (respect politeness)\n   - Central URL frontier or distributed\n\n7. Politeness Policies:\n   - Robots.txt: Which paths are allowed\n   - Robots-noindex: Don't index page\n   - Robots-nofollow: Don't follow links\n   - Crawl delay: Minimum wait between requests\n   - User-agent specific rules\n   - Cache robots.txt (domains change rules rarely)\n\n8. Handling Redirects:\n   - Follow 301/302 redirects\n   - Update URL mapping\n   - Detect redirect loops\n   - Max redirect depth\n\n9. Fetching:\n   - Use persistent connections (HTTP keep-alive)\n   - Timeout: 10 seconds\n   - Handle network errors gracefully\n   - Respect robots.txt\n   - Set user-agent\n   - Handle gzip compression\n   - Parallel fetching (asynchronous)\n\n10. Parsing:\n    - Extract links (<a href>)\n    - Extract title, meta tags\n    - Extract structured data (JSON-LD, microdata)\n    - Normalize URLs (remove query params, fragments)\n    - Resolve relative URLs\n\n11. Content Storage:\n    - Store raw HTML (for later re-parsing)\n    - Store extracted content\n    - Store metadata (title, headers, links)\n    - Use distributed filesystem\n    - Compress content\n\n12. Challenges:\n    - Crawl traps: Infinite pages (calendars, search results)\n    - JavaScript-heavy sites (need JS rendering)\n    - Authentication required pages\n    - Rate limiting/blocking\n    - Freshness vs crawl time trade-off\n    - Language detection\n\n13. Optimization:\n    - Cache DNS lookups\n    - Connection pooling\n    - Asynchronous I/O\n    - Batch processing\n    - Deduplicate before fetching\n\n14. Estimation:\n    - Web size: 5-10 billion pages\n    - Average page size: 100KB\n    - Crawl frequency: Monthly\n    - QPS needed: 5 billion pages / 30 days ≈ 2000 pages/sec",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-014",
      "question_key": "design-parking-system",
      "title": "Design a Parking Lot System",
      "prompt": "Design an automated parking lot management system that manages availability, parking spot assignment, payment, and entrance/exit gates.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Track available spots\n   - Assign spots to vehicles\n   - Record entry/exit\n   - Calculate parking fee\n   - Process payment\n   - Display availability\n\n2. Non-Functional Requirements:\n   - Real-time updates\n   - High availability\n   - Handle peak traffic\n   - Accurate billing\n\n3. Data Model:\n   - ParkingLot(id, name, total_spots, address)\n   - ParkingSpot(id, level, spot_number, type, is_available)\n   - ParkingTicket(id, spot_id, vehicle_id, entry_time, exit_time, fare)\n   - Vehicle(id, license_plate, type, owner)\n   - Payment(id, ticket_id, amount, method, timestamp)\n\n4. Types of Spots:\n   - Compact\n   - Large\n   - Handicap\n   - EV charging\n   - Different pricing\n\n5. Spot Assignment:\n   - Find nearest available spot\n   - Prefer close to entrance\n   - Level-based (find spot on level closest to entrance)\n   - OR hash-based (hash license plate → spot range)\n   - Minimize search time\n\n6. Core Operations:\n\n   Entrance Gate:\n   1. Vehicle arrives\n   2. Take photo/scan license plate\n   3. Check availability\n   4. Find parking spot\n   5. Generate ticket (paper or digital)\n   6. Open gate\n   7. Record entry time\n\n   Exit Gate:\n   1. Vehicle presents ticket\n   2. Retrieve entry time\n   3. Calculate fare\n   4. Process payment\n   5. Open gate\n   6. Update spot as available\n\n7. Pricing:\n   - Hourly rate\n   - Flat rate\n   - Peak/off-peak pricing\n   - Discounts\n   - Monthly passes\n\n8. Database Design:\n   - Normalize for consistency\n   - Index on (lot_id, is_available)\n   - Index on (license_plate) for quick lookup\n   - Partition by lot for scalability\n\n9. Availability Display:\n   - Real-time count of available spots per level\n   - Cache (update every 30 seconds)\n   - Show on app and entrance sign\n   - Refresh on spot change\n\n10. Spot Tracking:\n    - Sensors on each spot (optical or magnetic)\n    - Update available status when vehicle parks/leaves\n    - Handle sensor failures (assume occupied)\n    - Sync with database\n\n11. Challenges:\n    - Concurrent requests at peak times\n    - Spot reservation then no-show\n    - Overstaying\n    - Payment failures\n    - License plate misreading\n    - Sensor failures\n\n12. Optimization:\n    - Cache frequently accessed data\n    - Async payment processing\n    - Queue requests during peak times\n    - Batch sensor updates\n\n13. Reporting:\n    - Revenue by time period\n    - Occupancy rates\n    - Spot utilization\n    - Payment failures\n    - System uptime",
      "category": "system-design",
      "difficulty": "medium"
    },
    {
      "id": "q-015",
      "question_key": "design-dropbox",
      "title": "Design Dropbox (File Storage & Sync)",
      "prompt": "Design a file storage and sync system like Dropbox that allows users to store files and sync across devices.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Upload/download files\n   - Sync across devices\n   - Version history\n   - Share files\n   - Delete files\n   - Handle conflicts\n   - Offline support\n\n2. Non-Functional Requirements:\n   - Low bandwidth usage (delta sync)\n   - Low latency\n   - Reliability (don't lose files)\n   - Privacy (encryption)\n   - Scalability\n\n3. Architecture:\n   - Client app (desktop/mobile)\n   - API server\n   - File storage (S3)\n   - Metadata DB (file info, versions)\n   - Message queue (sync events)\n   - Notification service\n\n4. Upload Process:\n   1. Client chunks large files\n   2. Compute hash (SHA256) for each chunk\n   3. Upload chunks (can be parallel)\n   4. Server deduplicates chunks (if same hash exists, reuse)\n   5. Combine chunks into file\n   6. Update metadata\n   7. Notify other clients\n\n5. Download Process:\n   1. Client requests file\n   2. Server returns file chunks\n   3. Client downloads in parallel\n   4. Combine and verify hash\n   5. Write to disk\n\n6. Sync Mechanism:\n   - Client watches local folder for changes\n   - On change, sync to server\n   - Server broadcasts change to other clients\n   - Other clients pull changes\n   - Server maintains change log\n\n7. File Versioning:\n   - Keep old versions (configurable retention)\n   - Store metadata of each version\n   - Version tree/DAG to handle branches\n   - Restore to previous version\n\n8. Conflict Resolution:\n   - If two devices modify same file:\n     * Keep both versions (duplicate the file)\n     * OR manual merge\n   - Use timestamps and vector clocks\n   - Last-write-wins as fallback\n\n9. Chunking Strategy:\n   - Content-defined chunking (CDC)\n   - Find chunk boundaries based on content\n   - Handles insertions without shifting chunks\n   - Better deduplication\n\n10. Deduplication:\n    - Hash chunks → identify duplicates\n    - Share chunk blocks across files\n    - Save storage and bandwidth\n    - Maintain reference count per chunk\n    - Delete chunk when last reference removed\n\n11. Database Schema:\n    - User(id, email, quota, used_storage)\n    - File(id, user_id, name, parent_folder_id, created_at, modified_at)\n    - FileVersion(id, file_id, hash, size, created_at)\n    - Chunk(id, hash, size, storage_key)\n    - FileChunk(file_version_id, chunk_id, seq)\n\n12. Encryption:\n    - Client-side encryption (user controls keys)\n    - End-to-end encryption (server can't read)\n    - Key derivation from password\n    - Per-file encryption key\n\n13. Bandwidth Optimization:\n    - Delta sync (only changed bytes)\n    - Compression\n    - Deduplication\n    - Only sync when needed (smart sync)\n\n14. Offline Support:\n    - Local cache of files\n    - Queue changes when offline\n    - Sync when back online\n    - Handle offline conflicts\n\n15. Challenges:\n    - Handling large files (GB+)\n    - Sync conflicts\n    - Bandwidth limits\n    - Storage quota enforcement\n    - Fast change detection\n    - Security (access control, malware)\n    - Scalability (billions of files)",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-016",
      "question_key": "design-chat-system",
      "title": "Design a Chat System",
      "prompt": "Design a real-time chat system that supports one-on-one and group messages with delivery guarantees and online presence.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Send/receive messages\n   - One-on-one and group chats\n   - Message history\n   - Read receipts\n   - Typing indicators\n   - Online/offline presence\n   - Media sharing\n\n2. Non-Functional Requirements:\n   - Real-time (low latency)\n   - Scalable (millions of users)\n   - Reliable (messages don't get lost)\n   - Consistency (eventual)\n\n3. Architecture:\n   - API Gateway\n   - Chat Service\n   - Presence Service\n   - Message Service\n   - Storage (SQL for metadata, NoSQL for messages)\n   - Message Queue\n   - WebSocket servers\n   - Notification Service\n\n4. Key Components:\n\n   WebSocket Connection:\n   - Maintain persistent connection to server\n   - Server pushes real-time messages\n   - Client sends messages via WebSocket\n   - Handle disconnections gracefully\n\n   Message Service:\n   - Store messages\n   - Index by conversation_id, timestamp\n   - Support pagination\n   - Delete messages\n\n   Presence Service:\n   - Track online/offline status\n   - Track last seen\n   - Broadcast presence changes\n\n5. Data Model:\n   - User(id, username, avatar)\n   - Conversation(id, type, members, created_at)\n   - Message(id, conversation_id, user_id, content, created_at, status)\n   - ReadReceipt(user_id, conversation_id, last_message_read_id, read_at)\n   - UserPresence(user_id, status, last_seen, device_info)\n\n6. Message Flow:\n   1. User sends message via WebSocket\n   2. Server validates (auth, rate limit)\n   3. Persist to DB\n   4. Publish to message queue\n   5. Workers pick up and push to recipients\n   6. Send ACK to sender\n   7. Recipient receives via WebSocket\n   8. Recipient sends delivery confirmation\n\n7. Scalability Challenges:\n\n   Connection Management:\n   - Millions of concurrent connections\n   - Load balance across multiple WebSocket servers\n   - Use sticky sessions (client connects to same server)\n   - Server keeps in-memory map: user_id → connection\n\n   Group Messages:\n   - Problem: Fan-out to all members\n   - Pub/Sub model:\n     * Create channel per conversation\n     * Members subscribe to channel\n     * Publish message to channel\n     * Server broadcasts to subscribers\n   - Use Redis Pub/Sub or message queue\n\n8. Message Ordering:\n   - Use timestamp + sequence number\n   - Store in message: server_timestamp, seq\n   - Sort by these in client\n   - Handle clock skew\n\n9. Delivery Guarantees:\n   - At-least-once: Message queue with retries\n   - Deduplication: Use message_id\n   - ACK from recipient\n\n10. Presence Tracking:\n    - Heartbeat: Client sends \"alive\" every 30s\n    - If no heartbeat for 2min, mark offline\n    - Broadcast presence changes (eventually consistent)\n    - Cache presence in Redis\n\n11. Typing Indicator:\n    - User starts typing → send typing event\n    - Server broadcasts to conversation members\n    - Use debouncing (wait 3s without keystroke)\n    - Send typing event only once per 3s\n\n12. Read Receipts:\n    - Client sends read confirmation\n    - Update DB and broadcast\n    - Show \"read at [time]\" to sender\n\n13. Message History:\n    - Pagination (fetch 50 messages at a time)\n    - Cursor-based pagination\n    - Index on (conversation_id, created_at)\n\n14. Storage:\n    - SQL for metadata (users, conversations)\n    - NoSQL (MongoDB, Cassandra) for messages\n    - Messages immutable (append-only)\n    - TTL for very old messages (archive)\n\n15. Challenges:\n    - Handling offline messages (store and forward)\n    - Group chat scaling (100K+ members)\n    - Message encryption\n    - Spam detection\n    - Rate limiting\n    - Connection failures\n    - Clock synchronization",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-017",
      "question_key": "design-recommendation-engine",
      "title": "Design a Recommendation Engine",
      "prompt": "Design a recommendation system that suggests relevant items (products, content, etc.) to users based on behavior and preferences.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Generate recommendations\n   - Personalized per user\n   - Real-time or batch\n   - Explain recommendation\n   - A/B testing\n\n2. Non-Functional Requirements:\n   - Low latency (<100ms)\n   - High accuracy\n   - Scalability\n   - Freshness\n\n3. Recommendation Approaches:\n\n   Content-Based:\n   - Recommend items similar to user's past likes\n   - Features: genre, author, price, rating\n   - Similarity: cosine similarity, Euclidean distance\n   - Pros: Works for new items\n   - Cons: Echo chamber (narrow recommendations)\n\n   Collaborative Filtering:\n   - Find similar users → recommend their items\n   - User-based: \"People similar to you liked X\"\n   - Item-based: \"Users who liked this also liked X\"\n   - Pros: Discovers new content\n   - Cons: Cold start (new users), sparsity\n\n   Hybrid:\n   - Combine multiple approaches\n   - Weight different signals\n\n4. Data Collection:\n   - Explicit feedback: Ratings, thumbs up\n   - Implicit feedback: Views, purchases, clicks\n   - Context: Time, device, location\n   - User profile: Age, preferences, demographics\n\n5. Feature Engineering:\n   - Item features: Category, price, description, tags\n   - User features: Preferences, history\n   - Interaction features: Recency, frequency, duration\n   - Context features: Time of day, device type\n\n6. Model Training:\n   - Batch learning: Train daily/weekly on historical data\n   - Online learning: Update incrementally\n   - Matrix factorization: Decompose user-item matrix\n   - Deep learning: Neural networks for complex patterns\n\n7. Ranking Strategies:\n   - Score all candidates\n   - Rank by score\n   - Apply filters (diversity, recency, business rules)\n   - Return top K\n\n8. Candidate Generation:\n   - Problem: Score all items is expensive (millions)\n   - Reduce candidates first:\n     * Recall-set: Most similar items (fast retrieval)\n     * Use embeddings + nearest neighbor search\n     * Use hashing or locality-sensitive hashing\n   - Then rank top candidates\n\n9. Embeddings:\n   - Learn vector representation per item\n   - Nearby vectors = similar items\n   - Use embeddings for fast similarity search\n   - Methods:\n     * Word2Vec for items (skip-gram, CBOW)\n     * Matrix factorization\n     * Neural networks\n\n10. Two-Stage Architecture:\n    Stage 1 - Candidate Generation:\n    - Collaborative filtering with embeddings\n    - Content-based similarity\n    - Return top 100-1000 candidates\n    \n    Stage 2 - Ranking:\n    - Complex model with many features\n    - Score candidates\n    - Apply diversity/filter rules\n    - Return top 10-20\n\n11. Diversity:\n    - Problem: Users want different recommendations\n    - Techniques:\n      * Return top-K with diversity (max marginal relevance)\n      * Blend categories\n      * Freshen results\n\n12. Cold Start Problem:\n    - New users: No history\n      * Use popular items\n      * Use demographic similarity\n      * Collect initial preferences\n    - New items: No interactions\n      * Content-based\n      * Show to random users (exploration)\n      * Collaborative filtering with metadata\n\n13. System Design:\n    - Offline training (Spark, TensorFlow)\n    - Store models in model server\n    - Online serving:\n      * Get user features\n      * Score candidates\n      * Rank results\n      * Return in <100ms\n    - Embeddings in fast storage (Redis, Faiss)\n    - A/B testing framework\n\n14. Database Design:\n    - User interactions: user_id, item_id, event_type, timestamp\n    - User features: cached in Redis\n    - Item embeddings: vector DB (Weaviate, Faiss, Milvus)\n    - Model versions: track model performance\n\n15. Challenges:\n    - Data sparsity (most user-item pairs are unobserved)\n    - Cold start\n    - Popularity bias (recommend popular items)\n    - Drift (user preferences change)\n    - Feedback loops (recommendations affect user behavior)\n    - Fairness (diverse recommendations, not just popular)\n    - Serendipity (surprising but relevant)",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-018",
      "question_key": "design-distributed-transaction",
      "title": "Design Distributed Transactions",
      "prompt": "Design a system to handle distributed transactions across multiple databases or services while maintaining ACID properties.",
      "guided_answer": "Key Considerations:\n\n1. Challenges of Distributed Transactions:\n   - Network delays/failures\n   - Partial failures (some nodes succeed, others fail)\n   - No global clock\n   - Can't easily lock across services\n\n2. Two-Phase Commit (2PC):\n   Phase 1 - Prepare:\n   - Coordinator sends \"prepare\" to participants\n   - Each participant locks resources and prepares\n   - Returns \"yes\" (can commit) or \"no\" (abort)\n   \n   Phase 2 - Commit:\n   - Coordinator collects votes\n   - If all \"yes\": send \"commit\" to all\n   - If any \"no\": send \"abort\" to all\n   - Participants execute based on decision\n   \n   Pros: ACID properties\n   Cons: Blocking (resources locked), slow, not partition tolerant\n\n3. Problems with 2PC:\n   - Synchronous: Slow\n   - Blocking: Resources locked during voting\n   - Not partition tolerant: If coordinator fails, participants blocked\n   - Network issues cause cascading failures\n\n4. Saga Pattern (Alternative):\n   - Break transaction into local transactions\n   - Execute in sequence\n   - Compensating transactions for rollback\n   \n   Example: Money transfer A→B\n   1. Debit from A (compensate: credit A)\n   2. Credit to B (compensate: debit B)\n   \n   Types:\n   \n   Choreography:\n   - Services communicate directly\n   - Each service triggers next\n   - Harder to follow/debug\n   \n   Orchestration:\n   - Central coordinator/saga orchestrator\n   - Tells services what to do\n   - Easier to understand\n   - Single point of failure (mitigate with replication)\n\n5. Saga Implementation:\n   - Store saga state (which steps completed)\n   - Idempotent operations (safe to retry)\n   - Compensating transactions\n   - Handle timeouts\n   - Eventual consistency\n\n6. Event Sourcing:\n   - Store all state changes as events\n   - Rebuild state by replaying events\n   - Compensating events instead of updates\n   - Good audit trail\n\n7. Handling Failures:\n   - Idempotency keys: Duplicate requests return same result\n   - Retry logic with exponential backoff\n   - Timeouts: Assume failure if no response\n   - Monitoring: Detect stuck transactions\n   - Manual intervention for stuck sagas\n\n8. Isolation in Sagas:\n   - Saga provides weaker isolation than 2PC\n   - Dirty reads possible (step A completes, B sees updated data, but saga later aborts)\n   - Techniques to mitigate:\n     * Semantic locking: Mark record as locked\n     * Commutative updates: Order doesn't matter\n     * Reordering steps: Do independent steps\n\n9. Database Consistency:\n   - Eventual consistency: System consistent after saga completes\n   - Intermediate states may be inconsistent\n   - Application must handle this\n   - Business logic must be saga-aware\n\n10. Monitoring Sagas:\n    - Track saga instances\n    - Alert on timeouts\n    - Dashboard showing stuck sagas\n    - Manual intervention procedures\n\n11. Compensation Strategy:\n    - Compensating transaction must succeed\n    - Make idempotent (safe to retry)\n    - Consider: Can always compensate?\n    - Example: Booking (easy to cancel) vs password change (can't undo)\n\n12. Ordering & Dependencies:\n    - Dependency graph of steps\n    - Execute independent steps in parallel\n    - Wait for dependencies\n\n13. Example: Saga for Booking\n    Saga: ReserveFlight + ReserveHotel + ChargeCard\n    \n    Step 1: Reserve flight\n    - Success: Proceed\n    - Failure: Abort entire saga\n    - Compensation: Cancel flight reservation\n    \n    Step 2: Reserve hotel\n    - Success: Proceed\n    - Failure: Compensate step 1 (cancel flight), abort\n    - Compensation: Cancel hotel\n    \n    Step 3: Charge card\n    - Success: Saga complete\n    - Failure: Compensate steps 1&2\n\n14. Challenges:\n    - Complexity (harder than 2PC)\n    - Debugging (multiple services)\n    - Testing (failure scenarios)\n    - Data consistency (eventual)\n    - Compensating transactions (must be possible)\n    - Distributed tracing needed\n\n15. When to Use What:\n    - 2PC: Single database or tightly coupled services\n    - Saga: Microservices, loose coupling, eventual consistency acceptable",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-019",
      "question_key": "design-metrics-monitoring",
      "title": "Design a Metrics & Monitoring System",
      "prompt": "Design a system to collect, store, and analyze metrics from services at scale (like Prometheus or Datadog).",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Collect metrics from services\n   - Store metrics time-series data\n   - Query metrics\n   - Visualize (dashboards)\n   - Alerting\n   - Aggregation\n\n2. Types of Metrics:\n   - Counter: Monotonically increasing (requests_total)\n   - Gauge: Current value that can go up/down (cpu_usage)\n   - Histogram: Distribution (request_duration_seconds)\n   - Summary: Percentiles (p50, p95, p99 latency)\n\n3. High-Level Architecture:\n   - Applications export metrics\n   - Scraper/collector pulls metrics\n   - Time-series database stores metrics\n   - Query engine\n   - Visualization (dashboards)\n   - Alerting engine\n\n4. Collection Methods:\n   - Pull (Prometheus): Scraper pulls metrics from /metrics endpoint\n     * Pros: Simpler, scraper knows what to do\n     * Cons: Scraper must reach all services\n   \n   - Push: Services push metrics to collector\n     * Pros: Service controls when to send\n     * Cons: Scraper doesn't know if service is up\n   \n   - Hybrid: Both push and pull\n\n5. Metrics Format:\n   - Prometheus: Text-based\n     ```\n     http_requests_total{method=\"POST\",handler=\"/api/comments\"} 1027 1395066363000\n     ```\n   - Each metric: name, labels, value, timestamp\n\n6. Time-Series Database (TSDB):\n   - Store time-series data efficiently\n   - Examples: Prometheus, InfluxDB, VictoriaMetrics\n   \n   Features:\n   - Compression (values change slowly)\n   - Fast range queries\n   - Aggregation\n   - Retention policies\n   - High cardinality (many unique label combinations)\n\n7. Storage Design:\n   - Time-series indexed by metric name + labels\n   - Stored in chunks (e.g., 2 hours per chunk)\n   - Compress within chunk\n   - WAL (Write-Ahead Log) for durability\n   - Sharding by metric name hash\n\n8. Scraping:\n   - Scrape interval: 15-30 seconds\n   - Timeout: 10 seconds\n   - Scrape failure: Mark service down\n   - Service discovery: Find services to scrape\n\n9. Service Discovery:\n   - Static config: List of endpoints\n   - Dynamic: Query service registry (Consul, Kubernetes)\n   - Auto-detect new services\n   - Load balance scrapers\n\n10. Aggregation:\n    - Raw metrics too granular\n    - Aggregate: Average, sum, percentile\n    - Pre-compute common aggregations\n    - Store at different granularities (1min, 5min, 1hr)\n\n11. Queries:\n    - PromQL (Prometheus Query Language)\n    - Examples:\n      * `rate(requests_total[5m])` - requests/sec\n      * `histogram_quantile(0.95, request_duration)` - p95 latency\n      * `sum by (job) (requests_total)` - aggregate by job\n    - Graph queries over time\n    - Alert on thresholds\n\n12. Alerting:\n    - Define alert rules\n    - Evaluate continuously\n    - Alert manager routes alerts\n    - Deduplicate (same alert on multiple instances)\n    - Group related alerts\n    - Send to multiple channels (Slack, PagerDuty, email)\n\n13. Cardinality Explosion:\n    - Problem: Too many unique label combinations\n    - Example: user_id as label with millions of values\n    - Causes: High memory, slow queries\n    - Solution: Limit labels, use high-cardinality storage (VictoriaMetrics)\n\n14. Retention:\n    - Store for 15 days (configurable)\n    - Older data: Archive or delete\n    - Different retention per metric\n    - Trade-off: Storage cost vs historical data\n\n15. High Availability:\n    - Scrape from multiple locations\n    - TSDB replication\n    - Distributed querying\n    - Load balance across instances\n\n16. Challenges:\n    - Cardinality explosion\n    - High write throughput (millions of metrics/sec)\n    - Memory usage\n    - Query performance at scale\n    - Long-term storage\n    - Backup/recovery",
      "category": "system-design",
      "difficulty": "medium"
    },
    {
      "id": "q-020",
      "question_key": "design-logging-system",
      "title": "Design a Centralized Logging System",
      "prompt": "Design a logging system that collects logs from all services, makes them searchable, and provides insights (like ELK stack).",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Collect logs from services\n   - Index logs for fast search\n   - Full-text search\n   - Visualize log data\n   - Long-term retention\n   - Alert on log patterns\n\n2. Log Types:\n   - Application logs: Info, warning, error, debug\n   - System logs: OS events\n   - Access logs: HTTP requests\n   - Security logs: Auth attempts, access control\n\n3. Architecture:\n   - Log agent (runs on each host)\n   - Log aggregator\n   - Search engine (Elasticsearch)\n   - Visualization (Kibana)\n   - Alerting\n   - Long-term storage\n\n4. Collection:\n   - Agent on each server (Filebeat, Logstash, Fluentd)\n   - Tail log files\n   - Parse logs\n   - Buffer locally\n   - Send to aggregator\n\n5. Aggregation:\n   - Central system receives logs\n   - Distribute to indexers\n   - Handle backpressure (logs coming faster than can process)\n   - Buffer with message queue (Kafka)\n\n6. Parsing/Enrichment:\n   - Extract fields: timestamp, level, message, service, host\n   - Structured logs (JSON) or parse unstructured\n   - Add metadata: hostname, datacenter, version\n   - Grok patterns for complex logs\n\n7. Elasticsearch (Search Engine):\n   - Distributed full-text search\n   - Indexes documents (each log entry)\n   - Inverted index for fast search\n   - Sharding for scalability\n   - Replication for availability\n   - TTL for auto-delete old logs\n\n8. Indexing Strategy:\n   - Index per day (delete old indices)\n   - OR index per service per day\n   - Allows easy retention management\n   - Shard per index: 3-5 shards\n\n9. Log Format:\n   - Structured JSON preferred\n   ```json\n   {\n     \"timestamp\": \"2024-01-15T10:30:45Z\",\n     \"level\": \"ERROR\",\n     \"service\": \"user-service\",\n     \"message\": \"Failed to fetch user\",\n     \"user_id\": 12345,\n     \"duration_ms\": 250,\n     \"stack_trace\": \"...\"\n   }\n   ```\n\n10. Storage:\n    - Hot storage (ES): Recent logs (7 days)\n    - Warm storage (ES): Older logs (30 days)\n    - Cold storage (S3): Archive (years)\n    - Compress old indices\n\n11. Retention Policy:\n    - Different by log type:\n      * Debug logs: 3 days\n      * Info/warning: 7 days\n      * Error: 30 days\n      * Security: 1 year\n    - Cost-benefit analysis\n\n12. Queries:\n    - Full-text search: `\"connection failed\"`\n    - Field search: `level:ERROR AND service:payment`\n    - Range: `timestamp >= \"2024-01-15\"`\n    - Aggregations: Count by service, by error type\n    - Trends: Errors over time\n\n13. Visualization (Kibana):\n    - Dashboards: Real-time monitoring\n    - Visualizations: Charts, graphs\n    - Discover: Browse logs\n    - Alerts on patterns\n    - Saved searches\n\n14. Scale Considerations:\n    - Volume: Billions of log entries/day\n    - Throughput: Million logs/sec\n    - Multiple indices for parallelism\n    - Sharding across many nodes\n    - Replication for availability\n\n15. Performance:\n    - Bulk indexing (batch logs)\n    - Pipeline parallel processing\n    - Query optimization (expensive operations)\n    - Caching frequent queries\n\n16. Challenges:\n    - Cardinality explosion (too many unique values)\n    - Cost (storage + compute expensive)\n    - Log spam (debug logs too verbose)\n    - Retention: What to keep vs delete\n    - Privacy: PII in logs (mask sensitive data)\n    - Security: Who can access logs\n    - Debugging: Finding relevant logs\n\n17. Best Practices:\n    - Structured logging (JSON)\n    - Consistent field names\n    - Log correlation (trace_id, request_id)\n    - Appropriate log levels\n    - Avoid logging PII\n    - Monitor log ingestion lag\n    - Archive old data",
      "category": "system-design",
      "difficulty": "medium"
    },
    {
      "id": "q-021",
      "question_key": "design-live-streaming",
      "title": "Design a Live Video Streaming Platform",
      "prompt": "Design a live video streaming service (like Twitch) that supports multiple concurrent streams with real-time delivery to thousands of viewers.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Broadcast live video\n   - Watch live streams\n   - Switch quality based on network\n   - Chat/comments\n   - Pause/rewind (DVR)\n   - Recording\n\n2. Non-Functional Requirements:\n   - Ultra-low latency (<5s ideally, <30s acceptable)\n   - High availability\n   - Scalable (thousands of concurrent viewers)\n   - High bandwidth\n\n3. Architecture:\n   - Broadcaster app (streams from OBS, FFmpeg)\n   - Ingest server (receives stream)\n   - Transcoding farm (multiple qualities)\n   - Origin server (stores chunks)\n   - Edge CDN (delivers to viewers)\n   - Viewer app\n\n4. Ingest:\n   - Broadcaster uploads via RTMP or WebRTC\n   - Ingest server receives\n   - Buffer briefly\n   - Send to transcoders immediately\n   - Distribute chunks\n   - Return latency: ~3-5 seconds\n\n5. Transcoding:\n   - Input stream: 1080p high bitrate\n   - Output: Multiple resolutions (720p, 480p, 360p, 240p)\n   - Multiple bitrates per resolution (quality levels)\n   - Hardware acceleration (GPU)\n   - Distributed: Many transcoders in parallel\n\n6. Streaming Protocol:\n   - RTMP (Real-Time Messaging Protocol): Ingest (broadcaster)\n   - HLS (HTTP Live Streaming): Playback (viewers)\n     * Segmented: 10 second chunks\n     * Playlist: Points to chunks\n     * Adaptive bitrate: Client picks quality\n   - DASH (Dynamic Adaptive Streaming): Alternative to HLS\n   - WebRTC: Ultra-low latency (5s), CPU intensive\n\n7. HLS Format:\n   - Chunking: Break stream into 10s segments\n   - Playlist: m3u8 file listing segments\n   ```\n   #EXTM3U\n   #EXT-X-VERSION:3\n   #EXT-X-TARGETDURATION:10\n   segment1.ts\n   segment2.ts\n   segment3.ts\n   ...\n   ```\n   - Client requests playlist, gets chunks\n   - Playlist updates as new segments ready\n\n8. Storage:\n   - Live chunks: Origin server (fast access)\n   - Rolling buffer: Keep last 1-2 hours (DVR)\n   - Recording: Archive to S3\n   - Cleanup: Delete chunks after broadcast\n\n9. CDN Delivery:\n   - Edge locations worldwide\n   - Cache chunks at edges\n   - Pull from origin on first request\n   - Deliver from edge to subsequent requests\n   - Geographic routing\n\n10. Viewer Experience:\n    - Load balancing: Distribute viewers\n    - Start buffering: Pre-load a few segments\n    - Adaptive bitrate: Switch quality on network change\n    - Smooth switching: Cross-fade between qualities\n\n11. Chat System:\n    - Lightweight WebSocket connection\n    - Separate from video stream\n    - Broadcast chat to all viewers\n    - Rate limit messages\n    - Moderation\n\n12. Recording:\n    - While streaming: Capture encoded chunks\n    - Combine segments into single file\n    - Store on S3\n    - Metadata: Title, duration, thumbnail\n    - VOD (Video on Demand) service\n\n13. Latency Analysis:\n    - Network latency: ~1-2s\n    - Encoding: ~2s\n    - Chunking: ~10s\n    - Buffering: ~3-5s\n    - Total: ~15-20s typical, <30s acceptable\n    - WebRTC: ~2-5s but more CPU intensive\n\n14. Scalability:\n    - Broadcaster side: Ingest servers + transcoders\n    - Viewer side: Edge CDN\n    - Chat: Message queue + Pub/Sub\n    - Separate services: Better independent scaling\n\n15. Challenges:\n    - Latency: Hard to minimize without sacrificing reliability\n    - Encoding cost: Expensive at scale\n    - Bitrate adaptation: Complex logic\n    - Chat at scale: Millions of messages\n    - Quality issues: Network congestion, encoder failure\n    - Licensing: Music licensing, copyright\n    - DVR storage: Expensive to keep live history\n    - Security: Stream protection, DRM",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-022",
      "question_key": "design-payment-system",
      "title": "Design a Payment Processing System",
      "prompt": "Design a payment processing system that handles transactions securely, supports multiple payment methods, and ensures reliability.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Process payments (credit card, debit, digital wallets)\n   - Handle refunds\n   - Transaction history\n   - Multiple currencies\n   - Fraud detection\n   - PCI compliance\n   - Reconciliation\n\n2. Non-Functional Requirements:\n   - High availability (payments can't go down)\n   - Low latency\n   - Security (encryption, no data breaches)\n   - Auditability (every transaction logged)\n   - Compliance (PCI-DSS)\n\n3. High-Level Flow:\n   1. User enters payment info\n   2. Encrypt and send to payment service\n   3. Validate payment method\n   4. Call payment processor (Stripe, PayPal)\n   5. Store result in DB\n   6. Return confirmation\n   7. Async: Settlement, reconciliation\n\n4. Payment Methods:\n   - Credit/debit cards\n   - Digital wallets (Apple Pay, Google Pay)\n   - Bank transfers\n   - PayPal\n   - Cryptocurrency\n\n5. Architecture:\n   - API Gateway\n   - Payment Service\n   - PSP (Payment Service Provider) gateway\n   - Fraud Detection Service\n   - Settlement Service\n   - Reconciliation Service\n   - Database (payment records)\n   - Message Queue\n\n6. Database Design:\n   - Transaction(id, user_id, amount, currency, status, created_at)\n   - Status: pending, success, failed, refunded\n   - PaymentMethod(id, user_id, type, encrypted_data)\n   - Refund(id, transaction_id, amount, reason, status)\n   - Index: (user_id, created_at) for transaction history\n   - Partition by date for performance\n\n7. Payment Flow:\n   1. Create transaction record (pending)\n   2. Encrypt payment method\n   3. Call PSP with encrypted data\n   4. PSP processes, returns status\n   5. Update transaction record\n   6. Return result to client\n   7. Publish event (payment_completed)\n\n8. Security:\n   - Never store raw card data\n   - Use tokenization: PSP returns token\n   - Store token, use for future payments\n   - Encrypt sensitive data\n   - TLS for network (https)\n   - PCI-DSS compliance\n   - Two-factor authentication\n\n9. Handling Failures:\n   - Network timeout: Call PSP status endpoint\n   - PSP failure: Retry with exponential backoff\n   - Database failure: Failover to replica\n   - Service degradation: Queue transactions, process later\n\n10. Idempotency:\n    - Problem: Network timeout, retry causes double charge\n    - Solution: Idempotency key (unique per transaction)\n    - Check if transaction with key already exists\n    - Return existing result\n\n11. Reconciliation:\n    - PSP settlement: 1-3 days\n    - Match PSP records with internal records\n    - Handle discrepancies (rare)\n    - Automated reports\n\n12. Fraud Detection:\n    - Real-time rules:\n      * Velocity checks (too many transactions quickly)\n      * Geolocation (transaction in different country)\n      * Amount threshold\n      * Card comparison\n    - ML models:\n      * Anomaly detection\n      * Pattern analysis\n      * Risk scoring\n    - 3D Secure: Additional verification\n\n13. Refunds:\n    - Refund request → verify transaction\n    - Check refund window (e.g., 90 days)\n    - Call PSP refund API\n    - Create refund record\n    - Update transaction status\n    - Async settlement (PSP returns funds to customer)\n\n14. Multiple Currencies:\n    - Store amount in base currency\n    - FX rates: Update daily from external service\n    - Conversion at transaction time\n    - Round correctly\n\n15. PCI-DSS Compliance:\n    - Never log card numbers\n    - Encrypt transmission\n    - Encrypt storage\n    - Access control\n    - Audit logs\n    - Regular security testing\n    - Use tokenization (removes PCI burden)\n\n16. Challenges:\n    - Ensuring exactly-once processing\n    - Handling network failures\n    - Cross-border payments (regulations)\n    - Chargeback disputes\n    - Latency (settlement delays)\n    - Cost (payment processing fees)\n    - Compliance burden",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-023",
      "question_key": "design-hotel-booking",
      "title": "Design a Hotel Booking System",
      "prompt": "Design a booking system for hotels that handles reservations, inventory management, pricing, and concurrent booking requests.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Search hotels by date/location\n   - Check availability\n   - Make booking\n   - Manage pricing/rates\n   - Manage reservations\n   - Cancellation\n   - Payment\n\n2. Non-Functional Requirements:\n   - Handle peak traffic (holidays)\n   - Data consistency (don't oversell)\n   - Low latency (quick response)\n   - High availability\n\n3. Entities:\n   - Hotel: Name, location, photos, amenities\n   - Room: Room number, type, capacity, amenities\n   - Reservation: Guest, dates, room, price, status\n   - Pricing: Rate per night per room type per date\n\n4. Database Schema:\n   ```\n   Hotel(id, name, address, city, country)\n   Room(id, hotel_id, room_number, type, capacity)\n   RoomRate(id, room_type_id, date, base_price)\n   Reservation(id, room_id, guest_name, check_in, \n               check_out, num_nights, total_price, status, created_at)\n   ```\n\n5. Search Flow:\n   1. User enters check-in, check-out, location\n   2. Query available rooms (not booked)\n   3. Get pricing for dates\n   4. Display results (sorted by price/rating)\n   5. User selects room\n   6. Return to booking page\n\n7. Availability Query:\n   - Find rooms where no reservation overlaps dates\n   ```sql\n   SELECT DISTINCT r.id\n   FROM Room r\n   WHERE r.hotel_id = ?\n   AND r.id NOT IN (\n     SELECT room_id FROM Reservation\n     WHERE check_in < check_out_date\n     AND check_out > check_in_date\n     AND status IN ('confirmed', 'pending')\n   )\n   ```\n\n8. Booking (Critical):\n   - Problem: Multiple users book same room simultaneously\n   - Solution: Database-level constraints + optimistic/pessimistic locking\n   \n   Pessimistic Locking:\n   1. Lock room row\n   2. Check availability\n   3. Insert reservation\n   4. Unlock\n   5. Return confirmation\n   \n   Optimistic Locking:\n   1. Read room availability\n   2. Check available\n   3. INSERT with unique constraint\n   4. Fails if race condition\n   5. Retry\n\n9. Atomic Booking:\n   ```sql\n   -- Ensure no overlapping reservations\n   INSERT INTO Reservation (...)\n   SELECT ... \n   WHERE room_id = ?\n   AND NOT EXISTS (\n     SELECT 1 FROM Reservation\n     WHERE room_id = ?\n     AND status IN ('confirmed', 'pending')\n     AND check_in < ?\n     AND check_out > ?\n   )\n   ```\n\n10. Pricing Strategy:\n    - Base rate per room type per night\n    - Dynamic pricing:\n      * Higher in peak season\n      * Lower on weekdays\n      * Last-minute deals\n    - Discounts: Groups, loyalty, early bird\n    - Calculate total: (base_rate × num_nights) + taxes + fees\n\n11. Concurrency Control:\n    - Row-level locks in DB\n    - Reservation status: tentative → confirmed → checked-in → completed\n    - Tentative hold: 15 minutes\n    - Release if not paid\n\n12. Cancellation Policy:\n    - Free cancellation: Until N days before\n    - Partial refund: Within N days\n    - No refund: Day of arrival\n    - Enforce in business logic\n\n13. Inventory Management:\n    - Track rooms per type per hotel\n    - Block rooms for maintenance\n    - Overbooking (rarely, for no-shows): Upgrade guests\n    - Reserved inventory (for special guests)\n\n14. Scalability:\n    - Search queries: Cache results (5 min TTL)\n    - Popular hotels: Separate read replicas\n    - Booking: Single writer (lock serialization)\n    - Sharding: By hotel or location\n    - Message queue for async tasks (payment, confirmation email)\n\n15. Cache Strategy:\n    - Cache hotel data (photos, amenities)\n    - Cache room type data\n    - Cache availability (with short TTL)\n    - Don't cache pricing (changes frequently)\n\n16. Payment Integration:\n    - Booking service calls Payment service\n    - Payment may take time\n    - Reservation status: confirmed only after payment success\n    - Timeout: Release room if payment fails\n\n17. Data Consistency:\n    - ACID transactions for bookings\n    - Eventual consistency for search results\n    - Refresh cache periodically\n\n18. Challenges:\n    - Handling peak load (holidays)\n    - Inventory synchronization (multiple channels)\n    - Overbooking disputes\n    - Payment failures\n    - Time zone handling\n    - Different cancellation policies\n    - No-shows (guest booked but doesn't arrive)\n    - Double-bookings across systems",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-024",
      "question_key": "design-document-collaboration",
      "title": "Design a Document Collaboration System",
      "prompt": "Design a real-time collaborative document editing system (like Google Docs) that supports multiple users editing simultaneously.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Edit documents in real-time\n   - Multiple users simultaneously\n   - Auto-save\n   - Version history\n   - Undo/redo\n   - Comments/suggestions\n   - Permissions (view, edit, comment)\n\n2. Non-Functional Requirements:\n   - Low latency (<100ms)\n   - Handle concurrent edits\n   - Consistency (all users see same content)\n   - Offline support\n\n3. Core Challenge: Handling Concurrent Edits\n   - User A edits position 5\n   - User B edits position 10\n   - Both send updates\n   - Merge without conflicts\n   - Both users see same result\n\n4. Conflict Resolution Approaches:\n\n   Last-Write-Wins:\n   - Simple but loses data\n   - Not suitable for collaborative editing\n   \n   Operational Transformation (OT):\n   - Transform operations to work together\n   - Operation: Insert/Delete at position with content\n   - Two operations that conflict → adjust one to account for other\n   - Example:\n     * User A: Insert \"Hello\" at position 0\n     * User B: Insert \"World\" at position 0\n     * Transform B's operation: \"World\" now at position 5 (after \"Hello\")\n   \n   CRDT (Conflict-free Replicated Data Type):\n   - Mathematical structure ensuring consistency\n   - No central authority needed\n   - Can work offline\n   - Each edit gets unique ID + metadata\n   - Merge is commutative (order doesn't matter)\n   - Examples: Yjs, Automerge\n\n5. Architecture (OT-based):\n   - Document Server: Single authority\n   - Client App: Text editor\n   - WebSocket: Real-time sync\n   \n   Flow:\n   1. User makes edit (local)\n   2. Send operation to server\n   3. Server receives operation\n   4. Transform against pending operations\n   5. Apply to master document\n   6. Broadcast to other clients\n   7. Each client receives and transforms locally\n\n6. Architecture (CRDT-based):\n   - No central server needed\n   - Peer-to-peer or server assists\n   - Client: Full copy of document\n   - Edit: Create operation with unique ID\n   - Sync: Send operations, merge using CRDT\n   - Can handle offline edits\n\n7. Operational Transformation (OT) Details:\n   ```\n   Operation: {type: 'insert' | 'delete', position, content}\n   \n   Client state:\n   - Local document\n   - Pending operations (waiting for ACK)\n   - Version number (for OT)\n   \n   Server state:\n   - Master document\n   - Operation history\n   - Client versions\n   ```\n\n8. Handling Concurrent Operations:\n   ```\n   Server version 5: \"Hello\"\n   \n   Client A:\n   - Local version 5: \"Hello\"\n   - Insert \" World\" at position 5\n   - Send operation\n   \n   Client B:\n   - Local version 5: \"Hello\"\n   - Delete \"H\" at position 0\n   - Send operation\n   \n   Server receives both at version 5:\n   - Apply A's insert → \"Hello World\" (version 6)\n   - Transform B's delete: Delete at position 1 (adjusted)\n   - Apply B's delete → \"ello World\" (version 7)\n   - Broadcast both operations to clients\n   ```\n\n9. WebSocket Communication:\n   - Establish persistent connection\n   - Send operations in real-time\n   - Receive remote operations\n   - Maintain connection state\n   - Handle reconnection\n   - Offline queue for queued edits\n\n10. Storage:\n    - Document: Current text\n    - Snapshot: Periodic saves (every 100 operations)\n    - Operation log: Full edit history\n    - Versions: Track major versions\n    - Comments/suggestions: Separate collection\n\n11. Auto-save:\n    - Save periodically (every 30s)\n    - Save on major sections\n    - Save before closing\n    - Save locally immediately\n    - Sync with server asynchronously\n\n12. Undo/Redo:\n    - Undo: Reverse last operation\n    - Redo: Reapply reversed operation\n    - Handle: Local undo (simple) vs shared undo (complex)\n    - Per-user undo (only affects their edits)\n\n13. Version History:\n    - Store snapshots at intervals\n    - Keep operation log\n    - Replay operations to reconstruct any point in time\n    - Show diff between versions\n\n14. Comments/Suggestions:\n    - Attach comment to text range\n    - Associate with user\n    - Resolve when addressed\n    - Highlight in editor\n    - @mentions for notifications\n\n15. Permissions:\n    - Owner: Full access\n    - Editors: Can edit\n    - Commenters: Can comment only\n    - Viewers: Read-only\n    - Enforce on server\n\n16. Offline Support:\n    - Queue edits while offline\n    - Sync when back online\n    - Apply queued edits on top of changes\n    - Handle conflicts if document changed while offline\n\n17. Challenges:\n    - Consistency in presence of network partitions\n    - Scaling to millions of users\n    - Long documents (performance)\n    - Real-time collaboration at latency\n    - Malicious clients (prevent injection attacks)\n    - Rich text/formatting\n    - Mobile clients (low bandwidth)\n\n18. Libraries:\n    - Operational Transformation: google-diff-match-patch\n    - CRDT: Yjs, Automerge\n    - Framework: Firebase Realtime, Socket.io",
      "category": "system-design",
      "difficulty": "hard"
    },
    {
      "id": "q-025",
      "question_key": "design-ride-pooling",
      "title": "Design Ride Pooling (Carpooling System)",
      "prompt": "Design a system that matches multiple riders heading in the same direction to share a ride, optimizing cost and efficiency.",
      "guided_answer": "Key Considerations:\n\n1. Functional Requirements:\n   - Request ride\n   - Match with other riders\n   - Optimize route for multiple passengers\n   - Split fare fairly\n   - Pickup/dropoff multiple passengers\n   - Real-time location tracking\n   - Ratings and reviews\n\n2. Non-Functional Requirements:\n   - Low latency matching\n   - Scalable\n   - High availability\n   - Accurate routing\n\n3. Matching Problem:\n   - Challenge: Find compatible riders\n   - Constraints:\n     * Pickup/dropoff locations similar\n     * Time windows align\n     * Avoid large detours\n   - Goal: Maximize utilization (fill car)\n\n4. Matching Algorithm:\n   - Time window: Accept if pickup within T minutes\n   - Location matching:\n     * Pickup distance < D1 (e.g., 0.5 km)\n     * Dropoff distance < D2\n   - Route optimization:\n     * Calculate total distance with N passengers\n     * Accept if detour < D_max\n   - Run continuously (not instant match)\n\n5. Route Optimization:\n   - Traveling Salesman Problem variant\n   - Given: Driver location, pickup/dropoff locations\n   - Find: Optimal order to visit locations\n   - NP-hard: Use heuristics\n   - Greedy: Always go to nearest unvisited\n   - Genetic algorithms for better solution\n\n6. Fare Splitting:\n   - Base fare per rider\n   - Distance-based: Proportional to distance\n   - Fair split: Who benefits from pooling?\n   \n   Example:\n   - Solo UberX: $10\n   - UberPool (2 riders): $6 + $6 = $12 (split cost)\n   - Calculate: Original distance, actual distance with pool\n   - Driver gets bonus for pooling\n\n7. Wait Times:\n   - Rider A: Wait for Rider B to get picked up\n   - Acceptable wait: 5-10 minutes\n   - After wait threshold: Offer solo ride premium\n\n8. Pickup/Dropoff Order:\n   - Cannot just pick up in any order\n   - Optimize for minimal detour\n   - Consider traffic\n   - Pickup all first, then dropoff (simple)\n   - OR interleave (more complex but better)\n\n9. Database Design:\n   - RideRequest(id, rider_id, pickup, dropoff, timestamp, status)\n   - RidePool(id, driver_id, status, created_at)\n   - PoolRider(id, ride_pool_id, rider_id, order, pickup, dropoff)\n   - Payment(id, rider_id, ride_pool_id, amount, split_method)\n\n10. Matching Flow:\n    1. Rider requests ride\n    2. Find nearby drivers\n    3. Find other waiting riders heading same direction\n    4. Check constraints (time, location, detour)\n    5. Run route optimization\n    6. Offer match to riders\n    7. Accept/decline\n    8. Combine if both accept\n\n11. Architecture:\n    - Request Service: Handle new requests\n    - Matching Service: Find compatible riders\n    - Route Service: Optimize route (call Google Maps API)\n    - Driver Service: Assign rides\n    - Payment Service: Calculate fare, split\n    - Notification Service: Notify riders\n\n12. Geospatial Queries:\n    - Find drivers within radius\n    - Find riders heading same direction\n    - Use geospatial index (Redis GEO, MongoDB)\n    - or Quadtree/KD-tree\n\n13. Real-time Updates:\n    - Location updates every 5-10 seconds\n    - Route changes if traffic detected\n    - Recalculate ETA\n    - Notify riders of delay\n\n14. Cancellation:\n    - Rider can cancel before pickup\n    - Charge small fee (time-based)\n    - Driver can cancel if detour too large\n    - If one rider cancels, others still ride (adjust route)\n\n15. Challenges:\n    - Matching latency (must be fast)\n    - Fairness (some riders benefit more)\n    - Driver incentive (compensation for detour)\n    - Quality: Don't over-pool (long wait times)\n    - Reoptimization: Route changes with new rider\n    - Pickup/dropoff order: Complex optimization\n    - Rider experience: Some prefer privacy (no pooling)\n\n16. Optimization:\n    - Batch requests: Group before matching\n    - Prediction: Anticipate future requests\n    - Zone-based: Prefer riders in same zone\n    - Time of day: Pool more during peak hours\n    - Machine learning: Predict which riders will accept pool\n\n17. vs Solo Ride:\n    - Rider: Cheaper, longer wait, share space\n    - Driver: Slightly higher revenue (bonus + tips)\n    - City: Reduce congestion, emissions",
      "category": "system-design",
      "difficulty": "hard"
    }
  ]
}
